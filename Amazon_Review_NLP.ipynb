{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv('amazon_alexa.tsv', delimiter = '\\t', \n",
    "                      quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>\"Sometimes while playing a game, you can answe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>\"I have had a lot of fun with this thing. My 4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                      Love my Echo!         1  \n",
       "1                                          Loved it!         1  \n",
       "2  \"Sometimes while playing a game, you can answe...         1  \n",
       "3  \"I have had a lot of fun with this thing. My 4...         1  \n",
       "4                                              Music         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">feedback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>455.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2286.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feedback                                   \n",
       "          count mean  std  min  25%  50%  75%  max\n",
       "rating                                            \n",
       "1         161.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2          96.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3         152.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0\n",
       "4         455.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0\n",
       "5        2286.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('rating').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Engineering : New column to detect length odf reviw\n",
    "data['len_review'] = data['verified_reviews'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "      <th>len_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>\"Sometimes while playing a game, you can answe...</td>\n",
       "      <td>1</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>\"I have had a lot of fun with this thing. My 4...</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  len_review  \n",
       "0                                      Love my Echo!         1          13  \n",
       "1                                          Loved it!         1           9  \n",
       "2  \"Sometimes while playing a game, you can answe...         1         197  \n",
       "3  \"I have had a lot of fun with this thing. My 4...         1         174  \n",
       "4                                              Music         1           5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the texts\n",
    "corpus = []\n",
    "for i in range(0,3150):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['verified_reviews'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    ps=PorterStemmer()\n",
    "    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a bag of words\n",
    "cv = CountVectorizer(max_features=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = data.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20  34]\n",
      " [  3 573]]\n",
      "\n",
      "\n",
      "Accuracy for Logistic Regression: \n",
      " 0.94126984127\n"
     ]
    }
   ],
   "source": [
    "#Using Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix,accuracy_score)\n",
    "\n",
    "print(confusion_matrix(y_test,lr.predict(X_test)))\n",
    "print('\\n')\n",
    "print('Accuracy for Logistic Regression: \\n',accuracy_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12  42]\n",
      " [  0 576]]\n",
      "\n",
      "\n",
      "Accuracy for XGBoost: \n",
      " 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "#Using XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('\\n')\n",
    "print('Accuracy for XGBoost: \\n',accuracy_score(y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating bag of words model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000)\n",
    "X_tf = tfidf.fit_transform(corpus)\n",
    "y_tf = data.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tf, y_tf, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  52]\n",
      " [  0 576]]\n",
      "\n",
      "\n",
      "Accuracy for Logistic Regression: \n",
      " 0.91746031746\n"
     ]
    }
   ],
   "source": [
    "#Using Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_tf = LogisticRegression()\n",
    "lr_tf.fit(X_train,y_train)\n",
    "lr_tf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix,accuracy_score)\n",
    "\n",
    "print(confusion_matrix(y_test,lr_tf.predict(X_test)))\n",
    "print('\\n')\n",
    "print('Accuracy for Logistic Regression: \\n',accuracy_score(y_test, lr_tf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "vocab_cv = cv.vocabulary_\n",
    "print(len(vocab_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "vocab_tf = tfidf.vocabulary_\n",
    "print(type(vocab_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1500)\n"
     ]
    }
   ],
   "source": [
    "cv_coeffs = lr.coef_\n",
    "print(cv_coeffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2000)\n"
     ]
    }
   ],
   "source": [
    "tf_coeffs = lr_tf.coef_\n",
    "print(tf_coeffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12  42]\n",
      " [  0 576]]\n",
      "\n",
      "\n",
      "Accuracy for XGBoost: \n",
      " 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "#Using XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('\\n')\n",
    "print('Accuracy for XGBoost: \\n',accuracy_score(y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feat_dict(feat_list):\n",
    "    feat_dict={}\n",
    "    idx=0\n",
    "    for item in feat_list:\n",
    "        feat_dict[item] = idx\n",
    "        idx = idx+1\n",
    "    return feat_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_feat_dict = get_feat_dict(cv.get_feature_names())\n",
    "tf_feat_dict = get_feat_dict(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Contextual sentiment for count vector\n",
    "sentiList_cv = []\n",
    "threshold = 0.5\n",
    "for word, index in vocab_cv.items():\n",
    "    weight = cv_coeffs[0][cv_feat_dict[word]]\n",
    "    if weight > threshold or weight < - threshold:\n",
    "        sentiList_cv.append((word,weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiList_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suck', -0.79368416404904929),\n",
       " ('cannot', -0.73521816628823489),\n",
       " ('well', 1.4833375586097639),\n",
       " ('tin', -0.74060142893083669),\n",
       " ('nice', 1.024431009610399),\n",
       " ('cancel', -0.64193097779377994),\n",
       " ('cheap', -1.2248568218564291),\n",
       " ('tri', -0.96080516679581673),\n",
       " ('noth', -0.54817526703463992),\n",
       " ('speak', -1.2361787898561589),\n",
       " ('scare', -0.51521103027314064),\n",
       " ('learn', 0.55557991341972524),\n",
       " ('answer', -0.59954451127046171),\n",
       " ('enjoy', 1.3602428101131103),\n",
       " ('alway', -0.94001293247103779),\n",
       " ('back', -1.0712698130171625),\n",
       " ('annoy', 0.55728474382488014),\n",
       " ('right', 0.55769586562848095),\n",
       " ('thank', 0.65036722656779866),\n",
       " ('road', -0.68591407451541064),\n",
       " ('weather', 0.68008757758466565),\n",
       " ('hue', -0.55375636177067822),\n",
       " ('bose', -0.57093402835370222),\n",
       " ('screen', -0.50517478191641485),\n",
       " ('excel', 1.0091464789867797),\n",
       " ('tooth', 0.52431279349104953),\n",
       " ('unplug', -0.97759733361128409),\n",
       " ('hardli', -1.0650081427506832),\n",
       " ('took', 0.66755784911233784),\n",
       " ('spanish', -0.80213617491717826),\n",
       " ('daughter', 0.62591065034179549),\n",
       " ('cool', 0.76471181326247062),\n",
       " ('appar', -0.50759412775999968),\n",
       " ('ok', 0.57967613999182932),\n",
       " ('known', -0.55006434263965487),\n",
       " ('reboot', -0.54448271205084509),\n",
       " ('yet', 0.69127515582470223),\n",
       " ('awesom', 1.0844692153723636),\n",
       " ('love', 2.5247608185674322),\n",
       " ('keep', -0.50123365073584991),\n",
       " ('command', 0.66056674661233794),\n",
       " ('switch', -0.65532065348448421),\n",
       " ('espa', -0.53461566853477616),\n",
       " ('dot', -0.56638219130125955),\n",
       " ('littl', 0.62487473293146345),\n",
       " ('everi', 0.53512707842119855),\n",
       " ('randomli', -0.85784910105248724),\n",
       " ('repair', -0.69100541664506199),\n",
       " ('unnecessari', -0.75505858998314968),\n",
       " ('need', -0.519714490392577),\n",
       " ('stupid', -0.50757022131866403),\n",
       " ('password', -0.66887538060300533),\n",
       " ('nois', -0.61996751643695114),\n",
       " ('enabl', 0.59085688857220264),\n",
       " ('satisfi', 0.54921673596841702),\n",
       " ('like', 0.86775046061125849),\n",
       " ('fan', -0.71263369504656915),\n",
       " ('miss', -0.6690023527403548),\n",
       " ('wonder', 0.78428500528119993),\n",
       " ('cant', -0.84179380129525172),\n",
       " ('perfectli', 0.73722396562122061),\n",
       " ('common', -0.51426053135331995),\n",
       " ('none', -0.82928696283132508),\n",
       " ('disconnect', -0.68604903257417793),\n",
       " ('enter', -0.65121742314546249),\n",
       " ('reason', -0.77024904547640616),\n",
       " ('respond', -0.74398320161828246),\n",
       " ('adapt', -0.74230648387165221),\n",
       " ('month', -0.56696484492315646),\n",
       " ('upset', -0.78289120480683161),\n",
       " ('batteri', -0.6523970398080513),\n",
       " ('listen', 0.6472281645389476),\n",
       " ('read', 0.81740143847947233),\n",
       " ('goe', -0.7722622541529538),\n",
       " ('order', 0.52455185749410838),\n",
       " ('joke', -0.53395747159217966),\n",
       " ('control', 0.75116562218444183),\n",
       " ('came', -0.83357763888365055),\n",
       " ('good', 1.0668993144676475),\n",
       " ('spi', -0.6248614625971517),\n",
       " ('refurbish', -0.87564585645282444),\n",
       " ('alexi', -0.89733802788319483),\n",
       " ('forc', -0.52248576861249552),\n",
       " ('want', -0.86552761544837353),\n",
       " ('almost', -0.54660248011453971),\n",
       " ('much', -0.69171724654038391),\n",
       " ('loud', 0.52735106246604235),\n",
       " ('twice', -0.99421318190148522),\n",
       " ('origin', 0.65769798066614282),\n",
       " ('dumb', -0.87716827918966878),\n",
       " ('condit', 0.50685445606222301),\n",
       " ('handi', 0.56755433600690386),\n",
       " ('pair', -0.67099637930294209),\n",
       " ('half', -0.63669109769674959),\n",
       " ('would', -0.85385104097473985),\n",
       " ('night', -0.60512845228935053),\n",
       " ('return', -1.6101160976928468),\n",
       " ('easi', 1.803294967133013),\n",
       " ('famili', -0.52895686036490175),\n",
       " ('real', -0.56325366771318341),\n",
       " ('stick', 0.69296235354248514),\n",
       " ('ol', -0.53461566853477616),\n",
       " ('difficult', -1.1151181815793798),\n",
       " ('terribl', -1.7519793248798146),\n",
       " ('televis', 0.53653572657035664),\n",
       " ('motown', -0.5831111928618925),\n",
       " ('sad', -0.76014270833268338),\n",
       " ('gift', 0.73563883465272895),\n",
       " ('dont', -1.1415350548775958),\n",
       " ('load', -0.66260975618003426),\n",
       " ('fun', 0.99092483567984413),\n",
       " ('interact', -0.51486955039387716),\n",
       " ('siri', -0.88962853198513303),\n",
       " ('five', -0.50917690336065735),\n",
       " ('mode', -1.0320169463343862),\n",
       " ('bulb', -0.55816496424791007),\n",
       " ('review', -0.60070027265327919),\n",
       " ('garbag', -0.67318174612090698),\n",
       " ('pleas', 0.62209735819213241),\n",
       " ('meh', -0.7070580214375064),\n",
       " ('item', -0.86865763687502151),\n",
       " ('dislik', 0.65597119832429074),\n",
       " ('buld', -0.62224678796237576),\n",
       " ('low', -0.85591269492096866),\n",
       " ('realiz', -1.0180364680532488),\n",
       " ('follow', -1.0514129498527387),\n",
       " ('menu', -0.60308964245511965),\n",
       " ('poor', -1.6352312509288154),\n",
       " ('playlist', -0.6150002471725724),\n",
       " ('exactli', 0.57012750093513098),\n",
       " ('best', 0.97896697563206725),\n",
       " ('smooth', -0.65163295738868376),\n",
       " ('either', -0.67135343778024903),\n",
       " ('eas', 0.593983069742679),\n",
       " ('far', 0.69733988586579931),\n",
       " ('weird', -0.53683752069732771),\n",
       " ('make', -0.78482255957290536),\n",
       " ('chromecast', -0.50524691156805857),\n",
       " ('useless', -1.1975324656230484),\n",
       " ('alreadi', -0.97232101245605895),\n",
       " ('salsa', -0.5831111928618925),\n",
       " ('great', 2.0209605291525294),\n",
       " ('spent', -0.62816636005639959),\n",
       " ('volum', -0.74180415934809829),\n",
       " ('hot', -0.52274644106967072),\n",
       " ('perfect', 1.2589615254298661),\n",
       " ('option', 0.60377007130345406),\n",
       " ('featur', -0.75454147681903938),\n",
       " ('access', 0.61741950338055274),\n",
       " ('havent', -0.72757854579068981),\n",
       " ('work', -0.64668895977237995),\n",
       " ('aw', -0.94514158696135364),\n",
       " ('longer', -0.79714303228100203),\n",
       " ('expect', 1.3514651377785301),\n",
       " ('worthless', -1.1426640222308784),\n",
       " ('someth', -0.82630943833980408),\n",
       " ('amaz', 1.2176833110848515),\n",
       " ('cell', 0.52785761148334831),\n",
       " ('stop', -1.2889532272593089),\n",
       " ('set', 0.65078503320587933),\n",
       " ('applic', -0.59191095446647546),\n",
       " ('sale', 0.60871686218238685),\n",
       " ('room', 0.95131922730247864),\n",
       " ('blue', 0.63363614776302746),\n",
       " ('money', -0.92340780363262964),\n",
       " ('easier', 0.75699558375085174),\n",
       " ('could', 0.61177607563512859),\n",
       " ('sure', -0.71251250855886683)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiList_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contextual Sentiments for TfIdf Vectors\n",
    "sentiList_tf = []\n",
    "threshold = 0.5\n",
    "for word, index in vocab_tf.items():\n",
    "    weight = tf_coeffs[0][tf_feat_dict[word]]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        sentiList_tf.append((word, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiList_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suck', -0.87075754610411826),\n",
       " ('cannot', -0.78528422777689921),\n",
       " ('well', 1.1431913659154069),\n",
       " ('nice', 1.0315367982227033),\n",
       " ('wish', 0.649775371495665),\n",
       " ('everytim', -0.61973439756016568),\n",
       " ('cheap', -1.2944361090543091),\n",
       " ('tri', -1.8048567080658127),\n",
       " ('home', 0.57528988612136212),\n",
       " ('noth', -0.75702757574167567),\n",
       " ('disappoint', -0.53684386250724803),\n",
       " ('speak', -1.10543362189422),\n",
       " ('learn', 1.0282080246513838),\n",
       " ('smart', 0.58323325918794822),\n",
       " ('answer', -0.6663176240082318),\n",
       " ('enjoy', 1.2035594815599644),\n",
       " ('alway', -0.75364175436722658),\n",
       " ('back', -1.5977392942050153),\n",
       " ('hous', 0.7496623707849025),\n",
       " ('googl', -0.53680725013642516),\n",
       " ('thank', 0.69151167858512186),\n",
       " ('everyth', 0.86520925496932044),\n",
       " ('weather', 0.7591504839136558),\n",
       " ('hue', -0.64118860908756159),\n",
       " ('bose', -0.50776021421906925),\n",
       " ('screen', -1.0838977423113909),\n",
       " ('excel', 0.80939803830246659),\n",
       " ('wast', -0.58952554747076436),\n",
       " ('unplug', -0.95804724076149139),\n",
       " ('said', -0.59792952729532489),\n",
       " ('hardli', -1.0142188408477768),\n",
       " ('spanish', -0.93700600211047902),\n",
       " ('daughter', 0.50961102352327303),\n",
       " ('cool', 0.89353441140697532),\n",
       " ('massiv', -0.51019601323136909),\n",
       " ('emoji', -0.59646288820264037),\n",
       " ('reboot', -0.61235769919390137),\n",
       " ('card', -0.56876167903415742),\n",
       " ('connect', -0.55273973037610313),\n",
       " ('yet', 0.59898441059124996),\n",
       " ('awesom', 1.1650742999808961),\n",
       " ('love', 4.0910867996299265),\n",
       " ('switch', -0.6964776811393445),\n",
       " ('dot', -0.53598411499833187),\n",
       " ('littl', 0.82316734584954809),\n",
       " ('send', -0.63473048028162971),\n",
       " ('everi', 0.56302328503348975),\n",
       " ('randomli', -0.75703745865993788),\n",
       " ('live', 0.58908629469318585),\n",
       " ('repair', -0.9355721091986916),\n",
       " ('unrespons', -0.53646956325662964),\n",
       " ('reconnect', -0.62461972267984311),\n",
       " ('unnecessari', -0.52454537468542994),\n",
       " ('spoken', -0.65824970743419076),\n",
       " ('password', -0.64030969534623983),\n",
       " ('buy', -0.60026874528236451),\n",
       " ('nois', -0.53860904837155132),\n",
       " ('like', 1.6317390979808231),\n",
       " ('fan', -0.53937023821242791),\n",
       " ('wonder', 0.64566257074256317),\n",
       " ('cant', -0.93190716436409626),\n",
       " ('perfectli', 0.59397642355139035),\n",
       " ('none', -0.81782819646593596),\n",
       " ('disconnect', -0.73534197936261525),\n",
       " ('enter', -0.58092432354282131),\n",
       " ('reason', -0.83621774526535908),\n",
       " ('respond', -0.70402911310344085),\n",
       " ('use', 1.0767811689219842),\n",
       " ('month', -0.91104672661442632),\n",
       " ('upset', -0.69936175275652301),\n",
       " ('batteri', -0.60865687726599793),\n",
       " ('fix', -0.75960278245595136),\n",
       " ('listen', 0.88568392106840599),\n",
       " ('question', -0.55274130148223899),\n",
       " ('goe', -0.52811717254012214),\n",
       " ('new', 0.82917442499857796),\n",
       " ('control', 0.59844944040973436),\n",
       " ('came', -0.69086738712361839),\n",
       " ('scroll', -0.56923573526823201),\n",
       " ('youtub', -0.70583326616213271),\n",
       " ('good', 1.1592111455527165),\n",
       " ('echo', 0.85783749451189351),\n",
       " ('spi', -0.51960384142215732),\n",
       " ('refurbish', -1.1165354389189059),\n",
       " ('time', -1.2245319795517109),\n",
       " ('alexi', -0.71913694212927159),\n",
       " ('forc', -0.64545956573739804),\n",
       " ('want', -1.2539141538154246),\n",
       " ('much', -0.72480402059971094),\n",
       " ('amazon', -1.0249372352761845),\n",
       " ('twice', -1.1462254602101301),\n",
       " ('trust', -0.54161708114575569),\n",
       " ('origin', 0.63437209726873978),\n",
       " ('dumb', -0.77121317024643532),\n",
       " ('pair', -0.50877798153767639),\n",
       " ('half', -0.8714378493584849),\n",
       " ('would', -1.4236300653209391),\n",
       " ('still', 0.60214094449677558),\n",
       " ('return', -2.3959354235491106),\n",
       " ('easi', 2.2981035422623211),\n",
       " ('difficult', -0.90826582478906392),\n",
       " ('terribl', -1.8133049360198112),\n",
       " ('motown', -0.71609271198427282),\n",
       " ('sad', -0.72257549501524987),\n",
       " ('gift', 0.75025409416166933),\n",
       " ('dont', -0.8938514176805773),\n",
       " ('fun', 1.095389087331287),\n",
       " ('siri', -0.9226577493519561),\n",
       " ('mode', -0.79722030773173325),\n",
       " ('bulb', -0.7524363644080807),\n",
       " ('garbag', -0.73940963269224536),\n",
       " ('even', -0.67109888863862621),\n",
       " ('never', -0.64331239828399167),\n",
       " ('cycl', -0.54360159030058308),\n",
       " ('meh', -0.81425402946223779),\n",
       " ('item', -0.84092902237651412),\n",
       " ('dislik', 0.50956862262983471),\n",
       " ('buld', -0.55480991471466101),\n",
       " ('low', -0.79911318820951049),\n",
       " ('us', -0.77230443227599332),\n",
       " ('play', 0.58991585267227831),\n",
       " ('realiz', -1.2590706072242952),\n",
       " ('news', 0.51350111091549389),\n",
       " ('entertain', 0.53219815385249469),\n",
       " ('follow', -0.90102887679966337),\n",
       " ('menu', -0.53264717881682055),\n",
       " ('music', 1.2934718552083608),\n",
       " ('song', 0.55487885290459704),\n",
       " ('poor', -1.4899384717129169),\n",
       " ('middl', -0.55375691745545019),\n",
       " ('best', 1.0952888574618762),\n",
       " ('smooth', -0.52488075336252038),\n",
       " ('either', -0.71400258685636753),\n",
       " ('eas', 0.55989807638903799),\n",
       " ('far', 0.8626341992171499),\n",
       " ('absolut', 0.50395810873639291),\n",
       " ('make', -0.66605260591717963),\n",
       " ('useless', -1.1270745261020882),\n",
       " ('alreadi', -0.72675695705085863),\n",
       " ('salsa', -0.71609271198427282),\n",
       " ('great', 3.3550519172195044),\n",
       " ('spent', -0.64407267145505154),\n",
       " ('perfect', 1.2285104348413602),\n",
       " ('option', 0.50525190567569445),\n",
       " ('featur', -0.56063405973255376),\n",
       " ('havent', -0.52260580384436184),\n",
       " ('abl', 0.50571362509931572),\n",
       " ('work', -0.80438456128655234),\n",
       " ('aw', -0.67102251078809627),\n",
       " ('wall', -0.53335641638648734),\n",
       " ('longer', -0.88607772839774401),\n",
       " ('alarm', 0.54097572950526007),\n",
       " ('expect', 1.2615992122552824),\n",
       " ('worthless', -1.1760956529160851),\n",
       " ('amaz', 1.2277197590094562),\n",
       " ('defect', -0.51044890128630249),\n",
       " ('stop', -1.7812220437639124),\n",
       " ('set', 0.87591208573067736),\n",
       " ('plug', -0.7570639480850645),\n",
       " ('devic', -0.65838701050990811),\n",
       " ('room', 0.96655398744414733),\n",
       " ('speaker', 0.51718780480506465),\n",
       " ('money', -1.0014809736338781),\n",
       " ('easier', 0.55979191358198155),\n",
       " ('sure', -0.80353633025491356),\n",
       " ('regist', -0.58804571563114361)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiList_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suck',\n",
       " 'cannot',\n",
       " 'well',\n",
       " 'nice',\n",
       " 'wish',\n",
       " 'everytim',\n",
       " 'cheap',\n",
       " 'tri',\n",
       " 'home',\n",
       " 'noth',\n",
       " 'disappoint',\n",
       " 'speak',\n",
       " 'learn',\n",
       " 'smart',\n",
       " 'answer',\n",
       " 'enjoy',\n",
       " 'alway',\n",
       " 'back',\n",
       " 'hous',\n",
       " 'googl',\n",
       " 'thank',\n",
       " 'everyth',\n",
       " 'weather',\n",
       " 'hue',\n",
       " 'bose',\n",
       " 'screen',\n",
       " 'excel',\n",
       " 'wast',\n",
       " 'unplug',\n",
       " 'said',\n",
       " 'hardli',\n",
       " 'spanish',\n",
       " 'daughter',\n",
       " 'cool',\n",
       " 'massiv',\n",
       " 'emoji',\n",
       " 'reboot',\n",
       " 'card',\n",
       " 'connect',\n",
       " 'yet',\n",
       " 'awesom',\n",
       " 'love',\n",
       " 'switch',\n",
       " 'dot',\n",
       " 'littl',\n",
       " 'send',\n",
       " 'everi',\n",
       " 'randomli',\n",
       " 'live',\n",
       " 'repair',\n",
       " 'unrespons',\n",
       " 'reconnect',\n",
       " 'unnecessari',\n",
       " 'spoken',\n",
       " 'password',\n",
       " 'buy',\n",
       " 'nois',\n",
       " 'like',\n",
       " 'fan',\n",
       " 'wonder',\n",
       " 'cant',\n",
       " 'perfectli',\n",
       " 'none',\n",
       " 'disconnect',\n",
       " 'enter',\n",
       " 'reason',\n",
       " 'respond',\n",
       " 'use',\n",
       " 'month',\n",
       " 'upset',\n",
       " 'batteri',\n",
       " 'fix',\n",
       " 'listen',\n",
       " 'question',\n",
       " 'goe',\n",
       " 'new',\n",
       " 'control',\n",
       " 'came',\n",
       " 'scroll',\n",
       " 'youtub',\n",
       " 'good',\n",
       " 'echo',\n",
       " 'spi',\n",
       " 'refurbish',\n",
       " 'time',\n",
       " 'alexi',\n",
       " 'forc',\n",
       " 'want',\n",
       " 'much',\n",
       " 'amazon',\n",
       " 'twice',\n",
       " 'trust',\n",
       " 'origin',\n",
       " 'dumb',\n",
       " 'pair',\n",
       " 'half',\n",
       " 'would',\n",
       " 'still',\n",
       " 'return',\n",
       " 'easi',\n",
       " 'difficult',\n",
       " 'terribl',\n",
       " 'motown',\n",
       " 'sad',\n",
       " 'gift',\n",
       " 'dont',\n",
       " 'fun',\n",
       " 'siri',\n",
       " 'mode',\n",
       " 'bulb',\n",
       " 'garbag',\n",
       " 'even',\n",
       " 'never',\n",
       " 'cycl',\n",
       " 'meh',\n",
       " 'item',\n",
       " 'dislik',\n",
       " 'buld',\n",
       " 'low',\n",
       " 'us',\n",
       " 'play',\n",
       " 'realiz',\n",
       " 'news',\n",
       " 'entertain',\n",
       " 'follow',\n",
       " 'menu',\n",
       " 'music',\n",
       " 'song',\n",
       " 'poor',\n",
       " 'middl',\n",
       " 'best',\n",
       " 'smooth',\n",
       " 'either',\n",
       " 'eas',\n",
       " 'far',\n",
       " 'absolut',\n",
       " 'make',\n",
       " 'useless',\n",
       " 'alreadi',\n",
       " 'salsa',\n",
       " 'great',\n",
       " 'spent',\n",
       " 'perfect',\n",
       " 'option',\n",
       " 'featur',\n",
       " 'havent',\n",
       " 'abl',\n",
       " 'work',\n",
       " 'aw',\n",
       " 'wall',\n",
       " 'longer',\n",
       " 'alarm',\n",
       " 'expect',\n",
       " 'worthless',\n",
       " 'amaz',\n",
       " 'defect',\n",
       " 'stop',\n",
       " 'set',\n",
       " 'plug',\n",
       " 'devic',\n",
       " 'room',\n",
       " 'speaker',\n",
       " 'money',\n",
       " 'easier',\n",
       " 'sure',\n",
       " 'regist']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word[0] for word in sentiList_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polarized_dict = get_feat_dict(list(set([word[0] for word in sentiList_tf]).intersection(set([word[0] for word in sentiList_cv]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model with Count Vectors on polarized words\n",
    "cv_polarized = CountVectorizer(vocabulary = polarized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cv_polarized.fit_transform(corpus).toarray()\n",
    "y = data.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_cv_polarized = LogisticRegression()\n",
    "lr_cv_polarized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred_cv_polarized = lr_cv_polarized.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\n",
    "cm2 = confusion_matrix(y_test, y_pred_cv_polarized)\n",
    "ac2 = accuracy_score(y_test, y_pred_cv_polarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for LR: \n",
      " [[ 12  42]\n",
      " [  0 576]]\n",
      "\n",
      "\n",
      "Accuracy for LR: \n",
      " 0.934920634921\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for LR: \\n',cm)\n",
    "print('\\n')\n",
    "print('Accuracy for LR: \\n',ac2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  44]\n",
      " [  0 576]]\n",
      "\n",
      "\n",
      "Accuracy for XGBoost: \n",
      " 0.930158730159\n"
     ]
    }
   ],
   "source": [
    "#XGBoost for Contextual Sentiment\n",
    "#Using XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "classifier2 = XGBClassifier()\n",
    "classifier2.fit(X_train,y_train)\n",
    "y_pred = classifier2.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('\\n')\n",
    "print('Accuracy for XGBoost: \\n',accuracy_score(y_test, classifier2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Others\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Flatten, LSTM, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Dropout, Activation, Input, Dense, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "revdata = pd.read_csv('amazon_alexa.tsv', delimiter = '\\t', \n",
    "                      quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying the above function to column verified reviews\n",
    "revdata['verified_reviews'] = revdata['verified_reviews'].map(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>love echo !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>love it !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>sometim play game answer question correct alex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>lot fun thing old learn dinosaur control light...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                        love echo !         1  \n",
       "1                                          love it !         1  \n",
       "2  sometim play game answer question correct alex...         1  \n",
       "3  lot fun thing old learn dinosaur control light...         1  \n",
       "4                                              music         1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(revdata['verified_reviews'].values, revdata['feedback'], test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2577"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my new best friend ! ! i am littl behind time i have never anyth like this anoth alexa googl home super amaz use tool use play sleep sound sleep use amazon audibl book set alarm remind receiv notif play game her tell short stori lol pretti fun know peopl complain speaker i am sure whi i am sure open concept hous want blast music might blast around loud enough rent room bedroom perfect problem speaker love everyth it !\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2362\n"
     ]
    }
   ],
   "source": [
    "trainseq = tokenizer.texts_to_sequences(X_train)\n",
    "print(len(trainseq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainseq = pad_sequences(trainseq,maxlen=100,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "testseq = tokenizer.texts_to_sequences(X_test)\n",
    "testseq = pad_sequences(testseq,padding='post',maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2362, 100)\n",
      "(788, 100)\n"
     ]
    }
   ],
   "source": [
    "print(trainseq.shape)\n",
    "print(testseq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op_units = revdata['feedback'].nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size,emb_size,input_length=100))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 16)           41232     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                16010     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 57,253\n",
      "Trainable params: 57,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 4s - loss: 0.3471 - acc: 0.9005\n",
      "Epoch 2/5\n",
      " - 1s - loss: 0.2677 - acc: 0.9149\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.2388 - acc: 0.9183\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.1799 - acc: 0.9352\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.1160 - acc: 0.9610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x41068d82e8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(trainseq, y_train, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.162841\n",
      "Accuracy: 93.781726\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(testseq, y_test, verbose=2)\n",
    "print('Loss: %f' % (loss))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_corpus = revdata['verified_reviews'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_corpus = []\n",
    "for i in range(len(revdata['verified_reviews'])):\n",
    "    review = revdata['verified_reviews'][i].lower()\n",
    "    review = review.split()\n",
    "    review_corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'echo', '!']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hi\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Data processing\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=review_corpus, size=100, \n",
    "                window=5, workers=4, min_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 89\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(\"Vocabulary size: %d\"% len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'review_w2v.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pretrained w2v as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index_w2v = {}\n",
    "f = open(os.path.join('','review_w2v.txt'), encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coeffs = np.asarray(values[1:])\n",
    "    embeddings_index_w2v[word] = coeffs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review_corpus)\n",
    "seqs = tokenizer.texts_to_sequences(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2944 unique tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3150, 50)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %d unique tokens\"% len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(seqs, maxlen = 50)\n",
    "review_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2945\n"
     ]
    }
   ],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix_w2v = np.zeros((num_words, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index_w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embeddings_index_w2v will be all zeros\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "print(num_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            100,\n",
    "                            embeddings_initializer = Constant(embedding_matrix_w2v),\n",
    "                            input_length=100,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 100)          294500    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                100010    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 394,521\n",
      "Trainable params: 100,021\n",
      "Non-trainable params: 294,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 0.2509 - acc: 0.9229\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.2444 - acc: 0.9234\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.2382 - acc: 0.9242\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.2332 - acc: 0.9255\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.2290 - acc: 0.9255\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.2242 - acc: 0.9263\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.2206 - acc: 0.9280\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.2161 - acc: 0.9297\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.2146 - acc: 0.9289\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.2086 - acc: 0.9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x410b84bcf8>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(trainseq, y_train, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.258423\n",
      "Accuracy: 93.020305\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(testseq, y_test, verbose=2)\n",
    "print('Loss: %f' % (loss))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
